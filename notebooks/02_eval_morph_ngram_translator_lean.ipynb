{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f32a952",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Inputs\n",
    "TEST_HU_PATH   = Path(\"./data_v1/test_set_hu.txt\")                 # 1 HU sentence per line\n",
    "NGRAM_MAP_CSV  = Path(\"./data_v1/coverage/morph_ngram_dp_dict_v1.csv\")\n",
    "DATA_DIR = Path(\"./data_v1\")\n",
    "AUDIO_DIR = DATA_DIR / \"audio_morph_dp_v1\"\n",
    "AUDIO_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# emtsv binary\n",
    "EMTSV_EXEC     = \"emtsv\"   # or \"~/.local/bin/emtsv\"\n",
    "\n",
    "# Fallback behavior for unseen n‑grams\n",
    "USE_FALLBACK_ON_MISSING_NGRAM = True\n",
    "MAX_NGRAM = 3                          # 1..3\n",
    "\n",
    "# Cost weights\n",
    "LENGTH_GAP_W = 0.6     # |len(EN)-len(HU)| in phones\n",
    "EN_LEN_W     = 0.08    # per‑EN‑phone penalty\n",
    "RARITY_W     = 0.25    # penalty for low‑frequency EN words\n",
    "INS_COST     = 0.7     # insertion cost in phone‑DP\n",
    "DEL_COST     = 0.7     # deletion cost in phone‑DP\n",
    "\n",
    "# Outputs\n",
    "OUT_CSV = Path(\"./data_v1/results/results_morph_ngram_dp_v1.csv\")\n",
    "OUT_SUMMARY = Path(\"./data_v1/results/results_morph_ngram_summary_dp_v1.txt\")\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d7e64",
   "metadata": {},
   "source": [
    "## Imports & project wiring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f9d4aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liviusz/diploma/hu_en_homophonic/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, os, re, math, json, subprocess, shlex, pandas as pd\n",
    "from typing import List, Tuple, Sequence, Optional, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Add project src/\n",
    "THIS = Path.cwd()\n",
    "SRC  = (THIS / \"./src\").resolve()\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.append(str(SRC))\n",
    "\n",
    "# Project modules\n",
    "from src.g2p_hu import hu_text_to_ipa\n",
    "from src.cmudict_utils import load_cmudict\n",
    "from src.ipa_map import proxies_for_hu_phone\n",
    "from src.phone_mapping import phone_distance\n",
    "from src.metrics import wer, cer, per\n",
    "from src.tts_asr import synthesize_en_text, transcribe_hu\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from wordfreq import zipf_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732fc071",
   "metadata": {},
   "source": [
    "## emMorph (finest) segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f108050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _run(cmd: str, text: str):\n",
    "    return subprocess.run(\n",
    "        shlex.split(cmd),\n",
    "        input=(text if text.endswith(\"\\n\") else text + \"\\n\").encode(\"utf-8\"),\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True\n",
    "    )\n",
    "\n",
    "def _emtsv_lines(text: str, emtsv_exec: str):\n",
    "    tries = [\n",
    "        f\"{emtsv_exec} tok,emMorph --output-header\",\n",
    "        f\"{emtsv_exec} emMorph --output-header\",\n",
    "    ]\n",
    "    alt = os.path.expanduser(\"~/.local/bin/emtsv\")\n",
    "    if emtsv_exec != alt:\n",
    "        tries += [f\"{alt} tok,emMorph --output-header\", f\"{alt} emMorph --output-header\"]\n",
    "    last_err = None\n",
    "    for cmd in tries:\n",
    "        try:\n",
    "            p = _run(cmd, text)\n",
    "            return p.stdout.decode(\"utf-8\", errors=\"replace\").splitlines()\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"emtsv failed; last error: {last_err}\")\n",
    "\n",
    "def _parse_tsv(lines: List[str]):\n",
    "    if not lines: return [], []\n",
    "    header = [h.strip() for h in lines[0].split(\"\\t\")]\n",
    "    rows = []\n",
    "    for line in lines[1:]:\n",
    "        if not line.strip():\n",
    "            rows.append(None)\n",
    "            continue\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) < len(header):\n",
    "            parts += [\"\"] * (len(header)-len(parts))\n",
    "        rows.append(dict(zip(header, parts)))\n",
    "    return header, rows\n",
    "\n",
    "def _segments_from_morphana_string(morphana: str) -> List[str]:\n",
    "    segs = []\n",
    "    for chunk in morphana.split(\"+\"):\n",
    "        chunk = chunk.strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "        if \"=\" in chunk:\n",
    "            _, rhs = chunk.split(\"=\", 1)\n",
    "            rhs = rhs.strip()\n",
    "            if rhs:\n",
    "                segs.append(rhs)\n",
    "    return segs\n",
    "\n",
    "def _pick_finest_from_anas_json(anas_json: str) -> Optional[List[str]]:\n",
    "    try:\n",
    "        arr = json.loads(anas_json)\n",
    "        best = None; best_score = -1\n",
    "        for item in arr:\n",
    "            ma = item.get(\"morphana\") or \"\"\n",
    "            score = ma.count(\"+\")\n",
    "            if score > best_score and ma:\n",
    "                best_score = score; best = ma\n",
    "        if best:\n",
    "            return _segments_from_morphana_string(best)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def morph_segments_for_text(text: str, emtsv_exec: str = \"emtsv\") -> List[str]:\n",
    "    lines = _emtsv_lines(text, emtsv_exec)\n",
    "    header, rows = _parse_tsv(lines)\n",
    "    segs_all: List[str] = []\n",
    "    for row in rows:\n",
    "        if row is None:\n",
    "            continue\n",
    "        form = row.get(\"form\") or row.get(\"FORM\") or \"\"\n",
    "        anas = row.get(\"anas\") or row.get(\"ANAS\") or \"\"\n",
    "        segs = None\n",
    "        if anas.strip().startswith(\"[\") and \"morphana\" in anas:\n",
    "            segs = _pick_finest_from_anas_json(anas)\n",
    "        if not segs:\n",
    "            if form and any(ch.isalpha() for ch in form):\n",
    "                segs = [form]\n",
    "            else:\n",
    "                segs = []\n",
    "        segs_all.extend(segs)\n",
    "    return segs_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a25d6bb",
   "metadata": {},
   "source": [
    "## EN candidates & costs (for fallback scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7486c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a broad EN candidate set (moderate filter; used only when an n‑gram is missing from the map)\n",
    "@dataclass\n",
    "class EnCand:\n",
    "    word: str\n",
    "    pron: Tuple[str,...]\n",
    "    zipf: float\n",
    "    length: int\n",
    "    syll: int\n",
    "\n",
    "VOWELS = {\"i\",\"ɪ\",\"e\",\"ɛ\",\"æ\",\"ɑ\",\"a\",\"ɒ\",\"ʌ\",\"ɝ\",\"ə\",\"o\",\"ɔ\",\"ʊ\",\"u\",\"y\",\"ø\",\n",
    "          \"eɪ\",\"oʊ\",\"aɪ\",\"aʊ\",\"ɔɪ\",\"i:\",\"e:\",\"a:\",\"o:\",\"u:\",\"y:\",\"ø:\",\"iː\",\"eː\",\"aː\",\"oː\",\"uː\",\"yː\",\"øː\"}\n",
    "def is_vowel(p): return p in VOWELS\n",
    "def syllables(pron: Sequence[str]) -> int: return sum(1 for t in pron if is_vowel(t))\n",
    "\n",
    "cmu = load_cmudict(min_zipf=3.2, max_words=None)  # fairly broad\n",
    "EN_CANDS: List[EnCand] = []\n",
    "for w, prons in cmu.items():\n",
    "    if not prons: continue\n",
    "    p = tuple(prons[0])\n",
    "    if not re.fullmatch(r\"[a-z']+\", w or \"\"): continue\n",
    "    s = syllables(p)\n",
    "    if s > 2: continue\n",
    "    EN_CANDS.append(EnCand(w, p, zipf_frequency(w,\"en\"), len(p), s))\n",
    "\n",
    "EN_CANDS.sort(key=lambda x: (-x.zipf, x.syll, x.length))\n",
    "\n",
    "VOWELS_EN = {\"i\",\"ɪ\",\"eɪ\",\"ɛ\",\"æ\",\"ɑ\",\"ʌ\",\"ɝ\",\"ə\",\"oʊ\",\"ɔ\",\"ʊ\",\"u\",\"aɪ\",\"aʊ\",\"ɔɪ\"}\n",
    "def isv_en(p): return p in VOWELS_EN\n",
    "\n",
    "def phone_cost(en_p: str, hu_p: str, alpha=1.5, cv_pen=1.2) -> float:\n",
    "    best = math.inf\n",
    "    for en_proxy, pen in proxies_for_hu_phone(hu_p):\n",
    "        d = phone_distance(en_proxy, en_p)\n",
    "        if isv_en(en_proxy) != isv_en(en_p):\n",
    "            d += cv_pen\n",
    "        cand = alpha*d + pen\n",
    "        if cand < best:\n",
    "            best = cand\n",
    "    return float(best)\n",
    "\n",
    "def unit_cost_base(en_unit: Tuple[str,...], hu_unit: Tuple[str,...]) -> float:\n",
    "    if len(en_unit) == len(hu_unit):\n",
    "        if not hu_unit: return 0.0\n",
    "        return sum(phone_cost(e,h) for e,h in zip(en_unit, hu_unit)) / len(hu_unit)\n",
    "    m = min(len(en_unit), len(hu_unit))\n",
    "    if m == 0:\n",
    "        return 2.0 + LENGTH_GAP_W * abs(len(en_unit) - len(hu_unit))\n",
    "    base = sum(phone_cost(en_unit[i], hu_unit[i]) for i in range(m)) / m\n",
    "    return base + LENGTH_GAP_W * abs(len(en_unit) - len(hu_unit))\n",
    "\n",
    "def rarity_penalty(z: float) -> float:\n",
    "    return max(0.0, 4.0 - float(z)) * RARITY_W\n",
    "\n",
    "def unit_cost(en_unit: Tuple[str,...], hu_unit: Tuple[str,...], z: float) -> float:\n",
    "    return unit_cost_base(en_unit, hu_unit) + EN_LEN_W*len(en_unit) + rarity_penalty(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846b1f2",
   "metadata": {},
   "source": [
    "## Load n‑gram map & DP translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cf368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_ngram_map(csv_path: str | Path) -> Dict[int, Dict[Tuple[str,...], Tuple[str, Tuple[str,...], float]]]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    byn: Dict[int, Dict[Tuple[str,...], Tuple[str, Tuple[str,...], float]]] = {1:{},2:{},3:{}}\n",
    "    for _, r in df.iterrows():\n",
    "        n = int(r[\"n\"]); u = tuple(str(r[\"hu_ngram\"]).split(\" + \"))\n",
    "        w = str(r[\"en_word\"]); p = tuple(str(r[\"en_pron\"]).split())\n",
    "        c = float(r[\"cost\"])\n",
    "        byn[n][u] = (w, p, c)\n",
    "    return byn\n",
    "\n",
    "def hu_text_to_ipa_tokens(text: str) -> Tuple[str,...]:\n",
    "    iph = [p for p in hu_text_to_ipa(text) if isinstance(p, str) and p]\n",
    "    return tuple(iph)\n",
    "\n",
    "def best_en_for_ngram(morphs: Tuple[str,...]) -> Tuple[str, Tuple[str,...], float]:\n",
    "    hu_text = \"\".join(morphs)\n",
    "    hu_ipa  = hu_text_to_ipa_tokens(hu_text)\n",
    "    if not hu_ipa:\n",
    "        return \"\", tuple(), float(\"inf\")\n",
    "    # lightweight prefilter by length\n",
    "    prelim = []\n",
    "    for c in EN_CANDS:\n",
    "        prelim_cost = abs(c.length - len(hu_ipa)) + (c.syll > 1)*0.5\n",
    "        prelim.append((prelim_cost, c))\n",
    "    prelim.sort(key=lambda t: t[0])\n",
    "    shortlist = [c for _, c in prelim[:256]]  # beam\n",
    "    best_w, best_p, best_c = \"\", tuple(), float(\"inf\")\n",
    "    for c in shortlist:\n",
    "        cst = unit_cost(c.pron, hu_ipa, c.zipf)\n",
    "        if cst < best_c:\n",
    "            best_c = cst; best_w = c.word; best_p = c.pron\n",
    "    return best_w, best_p, best_c\n",
    "\n",
    "def dp_translate_morphs(morphs: List[str], ngram_map: Dict[int, Dict[Tuple[str,...], Tuple[str, Tuple[str,...], float]]],\n",
    "                        max_ngram: int = MAX_NGRAM, use_fallback: bool = USE_FALLBACK_ON_MISSING_NGRAM):\n",
    "    N = len(morphs)\n",
    "    dp = [ (float(\"inf\"), [], []) for _ in range(N+1) ]  # (cost, words, ks)\n",
    "    dp[0] = (0.0, [], [])\n",
    "    for i in range(N):\n",
    "        cur_cost, cur_words, cur_ks = dp[i]\n",
    "        if not math.isfinite(cur_cost): continue\n",
    "        for k in range(1, max_ngram+1):\n",
    "            j = i + k\n",
    "            if j > N: break\n",
    "            u = tuple(morphs[i:j])\n",
    "            if u in ngram_map.get(k, {}):\n",
    "                w, pr, c = ngram_map[k][u]\n",
    "            elif use_fallback:\n",
    "                w, pr, c = best_en_for_ngram(u)\n",
    "            else:\n",
    "                w, pr, c = \"uh\", tuple(), 2.0 + 0.5*(k-1)  # high cost\n",
    "            new_cost = cur_cost + c + 0.05  # small step penalty\n",
    "            if new_cost < dp[j][0]:\n",
    "                dp[j] = (new_cost, cur_words + [w], cur_ks + [k])\n",
    "    return dp[N]  # total_cost, words, ks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6243d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ngram_map = load_ngram_map(NGRAM_MAP_CSV)\n",
    "\n",
    "def eval_one(text: str,\n",
    "             do_tts_asr: bool = False,\n",
    "             idx: int = 0,\n",
    "             audio_dir = AUDIO_DIR):\n",
    "    row = {\"hu_text\": text}\n",
    "\n",
    "    # DP search\n",
    "    start_dp = time.perf_counter()\n",
    "    morphs = morph_segments_for_text(text, emtsv_exec=EMTSV_EXEC)\n",
    "\n",
    "    cost_morph, words_morph, ks = dp_translate_morphs(morphs, ngram_map, max_ngram=MAX_NGRAM, use_fallback=USE_FALLBACK_ON_MISSING_NGRAM)\n",
    "    en_text = \" \".join(words_morph)\n",
    "    elapsed_s_morph = time.perf_counter() - start_dp\n",
    "\n",
    "    row.update({\n",
    "        \"en_text_morph\": en_text,\n",
    "        \"morph_cost\": cost_morph,\n",
    "        \"time\": elapsed_s_morph\n",
    "    })\n",
    "\n",
    "    # Metrics via audio loop (optional)\n",
    "    if do_tts_asr:\n",
    "        # Morph\n",
    "        if en_text.strip():\n",
    "            wav_dp = audio_dir / f\"item_{idx:02d}_dp.wav\"\n",
    "            synthesize_en_text(en_text, str(wav_dp))\n",
    "            hyp_hu_dp = transcribe_hu(str(wav_dp))\n",
    "            row[\"hu_hyp_morph\"] = hyp_hu_dp\n",
    "            row[\"morph_wer\"] = wer(text.lower().split(), hyp_hu_dp.lower().split())\n",
    "            row[\"morph_cer\"] = cer(list(text.lower()), list(hyp_hu_dp.lower()))\n",
    "            row[\"morph_per\"] = per(hu_text_to_ipa(text), hu_text_to_ipa(hyp_hu_dp), sc=phone_distance)\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d972019",
   "metadata": {},
   "source": [
    "## Evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37140ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████| 28/28 [10:25<00:00, 22.33s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hu_text</th>\n",
       "      <th>en_text_morph</th>\n",
       "      <th>morph_cost</th>\n",
       "      <th>time</th>\n",
       "      <th>hu_hyp_morph</th>\n",
       "      <th>morph_wer</th>\n",
       "      <th>morph_cer</th>\n",
       "      <th>morph_per</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hány nyár</td>\n",
       "      <td>hands</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>2.453442</td>\n",
       "      <td>Hands.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Szép zöld fű</td>\n",
       "      <td>subjects</td>\n",
       "      <td>3.415000</td>\n",
       "      <td>11.838759</td>\n",
       "      <td>Subjects</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vár a víz</td>\n",
       "      <td>victims</td>\n",
       "      <td>3.267143</td>\n",
       "      <td>2.536568</td>\n",
       "      <td>Köszönöm, hogy megnézted!</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.444444</td>\n",
       "      <td>3.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gyúl a Gyertya</td>\n",
       "      <td>girlfriends</td>\n",
       "      <td>3.383333</td>\n",
       "      <td>3.090130</td>\n",
       "      <td>Göröfréns.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>1.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          hu_text en_text_morph  morph_cost       time  \\\n",
       "0       Hány nyár         hands    3.210000   2.453442   \n",
       "1    Szép zöld fű      subjects    3.415000  11.838759   \n",
       "2       Vár a víz       victims    3.267143   2.536568   \n",
       "3  Gyúl a Gyertya   girlfriends    3.383333   3.090130   \n",
       "\n",
       "                hu_hyp_morph  morph_wer  morph_cer  morph_per  \n",
       "0                     Hands.        1.0   0.777778   1.333333  \n",
       "1                   Subjects        1.0   0.916667   1.777778  \n",
       "2  Köszönöm, hogy megnézted!        1.0   2.444444   3.714286  \n",
       "3                 Göröfréns.        1.0   0.928571   1.777778  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load test HU sentences\n",
    "items = []\n",
    "if TEST_HU_PATH.exists():\n",
    "    with open(TEST_HU_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            parts = s.split(\".\", 1)\n",
    "            text = parts[1].strip() if len(parts) == 2 and parts[0].isdigit() else s\n",
    "            items.append(text)\n",
    "else:\n",
    "    # Fallback: tiny demo list if the file is missing (edit as needed)\n",
    "    items = [\"Sárga bögre, görbe bögre.\", \"Mit sütsz, kis szűcs?\"]\n",
    "print(f\"Loaded {len(items)} items.\")\n",
    "items[:3]\n",
    "\n",
    "rows = []\n",
    "for i, s in enumerate(tqdm(items, desc=\"Evaluating\")):\n",
    "    rows.append(eval_one(s, True, idx=i))\n",
    "\n",
    "OUT_CSV = Path(\"./data_v1/results/results_morph_ngram_dp_v1_partial2.csv\")\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a8329a",
   "metadata": {},
   "source": [
    "## Summary & diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2640d639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: 4\n",
      "WER: 2.5833\n",
      "CER: 4.7480\n",
      "PER: 3.2460\n",
      "data/audio_morph_dp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def fmt(x):\n",
    "    return \"NA\" if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))) else f\"{x:.4f}\"\n",
    "\n",
    "def summarize(df):\n",
    "    n = len(df)\n",
    "    per = df[\"morph_per\"].astype(float).mean()\n",
    "    cer = df[\"morph_cer\"].astype(float).mean()\n",
    "    wer = df[\"morph_wer\"].astype(float).mean()\n",
    "    lines = []\n",
    "    lines.append(f\"Examples: {n}\")\n",
    "    lines.append(f\"WER: {fmt(wer)}\")\n",
    "    lines.append(f\"CER: {fmt(cer)}\")\n",
    "    lines.append(f\"PER: {fmt(per)}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "OUT_SUMMARY.write_text(summarize(df), encoding=\"utf-8\")\n",
    "print(OUT_SUMMARY.read_text(encoding=\"utf-8\"))\n",
    "print(AUDIO_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0a69e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
