{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741e985e",
   "metadata": {},
   "source": [
    "\n",
    "# 01 — Morph **n‑gram** mapping (1–3 grams) + DP translator\n",
    "\n",
    "We keep **emMorph**’s **finest** segments as units (no coalescing), then build a dictionary that covers\n",
    "**unigrams, bigrams, trigrams** of morphs whenever a *compact* English word matches well.  \n",
    "At runtime, a **dynamic program** chooses between 1‑, 2‑ or 3‑morph mappings that yield the lowest\n",
    "phonetic cost for the sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bd4260",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83001441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# HU source\n",
    "CORPUS_DIR = Path(\"./data/hu_corpus\")   # *.txt, one sentence per line; if empty, falls back to wordfreq\n",
    "\n",
    "# emtsv\n",
    "EMTSV_EXEC = \"emtsv\"                     # or \"~/.local/bin/emtsv\"\n",
    "\n",
    "# Outputs\n",
    "OUT_DIR = Path(\"./data/coverage\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CSV_UNIGRAMS = OUT_DIR / \"hu_morph1_full.csv\"\n",
    "CSV_BIGRAMS  = OUT_DIR / \"hu_morph2_full.csv\"\n",
    "CSV_TRIGRAMS = OUT_DIR / \"hu_morph3_full.csv\"\n",
    "CSV_MAP      = OUT_DIR / \"morph_ngram_map.csv\"  # the final merged mapping\n",
    "\n",
    "# Coverage trimming by token mass\n",
    "TARGET_COVERAGE = 0.995\n",
    "\n",
    "# EN candidate pool settings\n",
    "MIN_ZIPF = 4.0        # drop rare/foreign words; raise to 3.8/4.0 if needed\n",
    "MAX_CAND = 50000      # cap EN candidates considered\n",
    "ALLOW_2SYLL_FOR_N = {1: False, 2: True, 3: True}\n",
    "LEN_RANGE_FOR_N = {1:(2,4), 2:(3,6), 3:(3,7)}  # phone length ranges per n-gram\n",
    "\n",
    "# Cost weights\n",
    "LENGTH_GAP_W = 0.6\n",
    "EN_LEN_W     = 0.08\n",
    "RARITY_W     = 0.25\n",
    "STEP_PENALTY = 0.05\n",
    "\n",
    "# DP limits\n",
    "NGRAM_MAX = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8f3ee",
   "metadata": {},
   "source": [
    "## Imports & project wiring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98b5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os, re, math, json, shlex, subprocess, collections, pandas as pd\n",
    "from typing import List, Dict, Tuple, Sequence, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# src/\n",
    "THIS = Path.cwd()\n",
    "SRC  = (THIS / \"./src\").resolve()\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.append(str(SRC))\n",
    "\n",
    "from src.g2p_hu import hu_text_to_ipa\n",
    "from src.cmudict_utils import load_cmudict\n",
    "from src.ipa_map import proxies_for_hu_phone\n",
    "from src.phone_mapping import phone_distance\n",
    "\n",
    "from wordfreq import top_n_list, zipf_frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3203685b",
   "metadata": {},
   "source": [
    "## emMorph (finest) segmentation → morph list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004f6d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ár', 'ad', 'ás']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _run(cmd: str, text: str):\n",
    "    return subprocess.run(\n",
    "        shlex.split(cmd),\n",
    "        input=(text if text.endswith(\"\\n\") else text + \"\\n\").encode(\"utf-8\"),\n",
    "        stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True\n",
    "    )\n",
    "\n",
    "def _emtsv_lines(text: str, emtsv_exec: str):\n",
    "    tries = [\n",
    "        f\"{emtsv_exec} tok,emMorph --output-header\",\n",
    "        f\"{emtsv_exec} emMorph --output-header\",\n",
    "    ]\n",
    "    alt = os.path.expanduser(\"~/.local/bin/emtsv\")\n",
    "    if emtsv_exec != alt:\n",
    "        tries += [f\"{alt} tok,emMorph --output-header\", f\"{alt} emMorph --output-header\"]\n",
    "    last_err = None\n",
    "    for cmd in tries:\n",
    "        try:\n",
    "            p = _run(cmd, text)\n",
    "            return p.stdout.decode(\"utf-8\", errors=\"replace\").splitlines()\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"emtsv failed; last error: {last_err}\")\n",
    "\n",
    "def _parse_tsv(lines: List[str]):\n",
    "    if not lines: return [], []\n",
    "    header = [h.strip() for h in lines[0].split(\"\\t\")]\n",
    "    rows = []\n",
    "    for line in lines[1:]:\n",
    "        if not line.strip():\n",
    "            rows.append(None)\n",
    "            continue\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) < len(header):\n",
    "            parts += [\"\"] * (len(header)-len(parts))\n",
    "        rows.append(dict(zip(header, parts)))\n",
    "    return header, rows\n",
    "\n",
    "def _segments_from_morphana_string(morphana: str) -> List[str]:\n",
    "    segs = []\n",
    "    for chunk in morphana.split(\"+\"):\n",
    "        chunk = chunk.strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "        if \"=\" in chunk:\n",
    "            _, rhs = chunk.split(\"=\", 1)\n",
    "            rhs = rhs.strip()\n",
    "            if rhs:\n",
    "                segs.append(rhs)\n",
    "    return segs\n",
    "\n",
    "def _pick_finest_from_anas_json(anas_json: str) -> Optional[List[str]]:\n",
    "    try:\n",
    "        arr = json.loads(anas_json)\n",
    "        best = None; best_score = -1\n",
    "        for item in arr:\n",
    "            ma = item.get(\"morphana\") or \"\"\n",
    "            score = ma.count(\"+\")\n",
    "            if score > best_score and ma:\n",
    "                best_score = score; best = ma\n",
    "        if best:\n",
    "            return _segments_from_morphana_string(best)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def morph_segments_for_text(text: str, emtsv_exec: str = \"emtsv\") -> List[str]:\n",
    "    lines = _emtsv_lines(text, emtsv_exec)\n",
    "    header, rows = _parse_tsv(lines)\n",
    "    segs_all: List[str] = []\n",
    "    for row in rows:\n",
    "        if row is None:\n",
    "            continue\n",
    "        form = row.get(\"form\") or row.get(\"FORM\") or \"\"\n",
    "        anas = row.get(\"anas\") or row.get(\"ANAS\") or \"\"\n",
    "        segs = None\n",
    "        if anas.strip().startswith(\"[\") and \"morphana\" in anas:\n",
    "            segs = _pick_finest_from_anas_json(anas)\n",
    "        if not segs:\n",
    "            if form and any(ch.isalpha() for ch in form):\n",
    "                segs = [form]\n",
    "            else:\n",
    "                segs = []\n",
    "        segs_all.extend(segs)\n",
    "    return segs_all\n",
    "\n",
    "# smoke test\n",
    "try:\n",
    "    print(morph_segments_for_text(\"Áradás.\", emtsv_exec=EMTSV_EXEC))\n",
    "except Exception as e:\n",
    "    print(\"NOTE: emtsv may not be available in this environment. Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a34ebf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simple Hungarian grapheme syllabifier and stem micro-splitter ---\n",
    "\n",
    "HU_VOWELS = set(list(\"aáeéiíoóöőuúüűAÁEÉIÍOÓÖŐUÚÜŰ\"))\n",
    "# Treat Hungarian digraphs/trigraphs as single consonant graphemes\n",
    "DIGRAPHS3 = [\"dzs\",\"DZS\"]\n",
    "DIGRAPHS2 = [\"dz\",\"cs\",\"gy\",\"ly\",\"ny\",\"sz\",\"ty\",\"zs\",\n",
    "             \"DZ\",\"CS\",\"GY\",\"LY\",\"NY\",\"SZ\",\"TY\",\"ZS\"]\n",
    "\n",
    "def _graphemes(word: str) -> list[str]:\n",
    "    \"\"\"Split Hungarian word into graphemes, keeping digraphs/trigraphs intact.\"\"\"\n",
    "    s = word\n",
    "    g = []\n",
    "    i = 0\n",
    "    while i < len(s):\n",
    "        # longest-first (dzs)\n",
    "        if i+3 <= len(s) and s[i:i+3] in DIGRAPHS3:\n",
    "            g.append(s[i:i+3]); i += 3; continue\n",
    "        if i+2 <= len(s) and s[i:i+2] in DIGRAPHS2:\n",
    "            g.append(s[i:i+2]); i += 2; continue\n",
    "        g.append(s[i]); i += 1\n",
    "    return g\n",
    "\n",
    "def _is_vowel_g(gr: str) -> bool:\n",
    "    # any of its letters is a vowel → treat grapheme as vowel\n",
    "    return any(ch in HU_VOWELS for ch in gr)\n",
    "\n",
    "def syllabify_hu_graphemes(word: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Very simple syllabification by graphemes:\n",
    "      - each syllable has exactly one vowel nucleus;\n",
    "      - apply 'maximal onset' with a conservative coda: keep at most 1 consonant in coda.\n",
    "    \"\"\"\n",
    "    g = _graphemes(word)\n",
    "    if not g: return []\n",
    "    # positions of vowel nuclei\n",
    "    nuclei = [i for i,gr in enumerate(g) if _is_vowel_g(gr)]\n",
    "    if not nuclei:\n",
    "        return [\"\".join(g)]  # all-consonant (rare), don't split\n",
    "\n",
    "    sylls: list[list[str]] = []\n",
    "\n",
    "    # handle onset before first nucleus\n",
    "    start = 0\n",
    "    for idx, nuc in enumerate(nuclei):\n",
    "        if idx == 0:\n",
    "            onset = g[:nuc]    # all leading consonants as onset of first syllable\n",
    "        else:\n",
    "            prev_nuc = nuclei[idx-1]\n",
    "            # between prev_nuc and nuc:\n",
    "            bridge = g[prev_nuc+1:nuc]\n",
    "            # keep at most 1 consonant as coda for previous syllable, rest goes to onset here\n",
    "            coda_prev = bridge[:1]\n",
    "            onset = bridge[1:]\n",
    "            # append coda to previous syllable\n",
    "            sylls[-1].extend(coda_prev)\n",
    "        sylls.append(onset + [g[nuc]])  # onset + nucleus\n",
    "\n",
    "    # tail after last nucleus → coda of last syllable\n",
    "    tail = g[nuclei[-1]+1:]\n",
    "    if tail:\n",
    "        # keep at most 2 in coda (Hungarian allows some clusters), tweak if you like\n",
    "        coda = tail[:2]\n",
    "        onset_extra = tail[2:]  # if any, attach to last coda conservatively\n",
    "        sylls[-1].extend(coda + onset_extra)\n",
    "\n",
    "    return [\"\".join(x) for x in sylls]\n",
    "\n",
    "# Micro-split long stems based on grapheme syllables\n",
    "REFINE_LONG_STEMS = True\n",
    "MAX_CHARS_PER_UNIT = 6   # if a morph is longer than this (characters), split into syllables\n",
    "MIN_KEEP_CHARS    = 3    # very short morphs are left as-is\n",
    "\n",
    "def refine_morph_units(morphs: list[str]) -> list[str]:\n",
    "    out: list[str] = []\n",
    "    for m in morphs:\n",
    "        if (not REFINE_LONG_STEMS) or (len(m) <= MAX_CHARS_PER_UNIT) or (len(m) < MIN_KEEP_CHARS):\n",
    "            out.append(m)\n",
    "            continue\n",
    "        # split into grapheme syllables\n",
    "        syls = syllabify_hu_graphemes(m)\n",
    "        # merge ultra-short sylls back to neighbors to avoid 1-char pieces\n",
    "        merged: list[str] = []\n",
    "        for s in syls:\n",
    "            if not merged:\n",
    "                merged.append(s)\n",
    "            elif len(s) == 1:\n",
    "                merged[-1] = merged[-1] + s\n",
    "            else:\n",
    "                merged.append(s)\n",
    "        out.extend(merged)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450257d",
   "metadata": {},
   "source": [
    "## Build morph n‑gram inventories (1–3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f823e789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines processed: 46637 uni: 12564 bi: 27949 tri: 24107\n",
      "Wrote: data/coverage/hu_morph1_full.csv rows: 12564\n",
      "Wrote: data/coverage/hu_morph2_full.csv rows: 27949\n",
      "Wrote: data/coverage/hu_morph3_full.csv rows: 24107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(  unit  count\n",
       " 0    t   4093\n",
       " 1    i   2263\n",
       " 2   ás   1816\n",
       " 3   et   1794\n",
       " 4   és   1557\n",
       " 5   ek   1523\n",
       " 6   el   1425\n",
       " 7   at   1348\n",
       " 8  meg   1282\n",
       " 9   ok   1236,\n",
       "        unit  count\n",
       " 0  tA/ + Ad    416\n",
       " 1     é + t    344\n",
       " 2     á + t    317\n",
       " 3    és + é    282\n",
       " 4     t + a    274\n",
       " 5   ek + et    253\n",
       " 6     t + e    248\n",
       " 7    ás + a    235\n",
       " 8    ás + á    224\n",
       " 9    és + e    220,\n",
       "                unit  count\n",
       " 0  tA/ + Ad + j]=té    165\n",
       " 1    Ad + j]=té + k    161\n",
       " 2  tA/ + Ad + j]=tá    156\n",
       " 3    Ad + j]=tá + k    154\n",
       " 4        ás + á + t    105\n",
       " 5        és + é + t     95\n",
       " 6       ás + ár + a     72\n",
       " 7       és + é + re     67\n",
       " 8   Ad + j]=ot + tá     62\n",
       " 9    j]=ot + tá + k     62)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def iter_source_lines():\n",
    "    if CORPUS_DIR.exists():\n",
    "        for p in sorted(CORPUS_DIR.glob(\"*.txt\")):\n",
    "            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line: yield line\n",
    "        return\n",
    "    if top_n_list is not None:\n",
    "        for w in top_n_list(\"hu\", 200000):\n",
    "            yield w\n",
    "        return\n",
    "    yield from [\"Sárga bögre, görbe bögre\", \"Mit sütsz, kis szűcs\", \"Árvíztűrő tükörfúrógép\"]\n",
    "\n",
    "def count_morph_ngrams(nmax=3, emtsv_exec: str = \"emtsv\"):\n",
    "    cnts = {1: collections.Counter(), 2: collections.Counter(), 3: collections.Counter()}\n",
    "    seen = 0\n",
    "    for line in iter_source_lines():\n",
    "        try:\n",
    "            morphs = morph_segments_for_text(line, emtsv_exec=emtsv_exec)\n",
    "            morphs = refine_morph_units(morphs)\n",
    "        except Exception:\n",
    "            morphs = []\n",
    "        L = len(morphs)\n",
    "        for i in range(L):\n",
    "            for n in (1,2,3):\n",
    "                if i+n <= L:\n",
    "                    seq = tuple(morphs[i:i+n])\n",
    "                    cnts[n][seq] += 1\n",
    "        seen += 1 \n",
    "    return cnts, seen\n",
    "\n",
    "cnts, seen = count_morph_ngrams(3, emtsv_exec=EMTSV_EXEC)\n",
    "print(\"Lines processed:\", seen, \"uni:\", len(cnts[1]), \"bi:\", len(cnts[2]), \"tri:\", len(cnts[3]))\n",
    "\n",
    "def write_top(counter, path: Path):\n",
    "    df = pd.DataFrame([{\"unit\": \" + \".join(u), \"count\": c} for u,c in counter.items()]).sort_values([\"count\",\"unit\"], ascending=[False, True]).reset_index(drop=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    cum = df[\"count\"].cumsum().astype(float); total=float(df[\"count\"].sum())\n",
    "    cut = (cum/ max(1.0,total) <= TARGET_COVERAGE)\n",
    "    if cut.any():\n",
    "        last = cut[cut].index.max()+1\n",
    "        df_top = df.iloc[:last].copy()\n",
    "    else:\n",
    "        df_top = df\n",
    "    return df, df_top\n",
    "\n",
    "df1, top1 = write_top(cnts[1], CSV_UNIGRAMS)\n",
    "df2, top2 = write_top(cnts[2], CSV_BIGRAMS)\n",
    "df3, top3 = write_top(cnts[3], CSV_TRIGRAMS)\n",
    "\n",
    "print(\"Wrote:\", CSV_UNIGRAMS, \"rows:\", len(df1))\n",
    "print(\"Wrote:\", CSV_BIGRAMS,  \"rows:\", len(df2))\n",
    "print(\"Wrote:\", CSV_TRIGRAMS, \"rows:\", len(df3))\n",
    "\n",
    "top1.head(10), top2.head(10), top3.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d95e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2894"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1 = top1.head(2894)\n",
    "top2 = top2.head(4040)\n",
    "top3 = top3.head(1633)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad3dd8",
   "metadata": {},
   "source": [
    "## Curated EN candidate pool (by n‑gram size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae4b7e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1372"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "VOWELS = {\n",
    "    \"i\",\"ɪ\",\"e\",\"ɛ\",\"æ\",\"ɑ\",\"a\",\"ɒ\",\"ʌ\",\"ɝ\",\"ə\",\"o\",\"ɔ\",\"ʊ\",\"u\",\"y\",\"ø\",\n",
    "    \"eɪ\",\"oʊ\",\"aɪ\",\"aʊ\",\"ɔɪ\",\"i:\",\"e:\",\"a:\",\"o:\",\"u:\",\"y:\",\"ø:\",\"iː\",\"eː\",\"aː\",\"oː\",\"uː\",\"yː\",\"øː\"\n",
    "}\n",
    "def is_vowel(p): return p in VOWELS\n",
    "def syllables(pron: Sequence[str]) -> int: return sum(1 for t in pron if is_vowel(t))\n",
    "\n",
    "def onset_coda(pron: Sequence[str]):\n",
    "    j = next((i for i,p in enumerate(pron) if is_vowel(p)), None)\n",
    "    if j is None: return (), None, ()\n",
    "    return tuple(pron[:j]), j, tuple(pron[j+1:])\n",
    "\n",
    "ALLOWED_ONSET_1 = {\"p\",\"b\",\"t\",\"d\",\"k\",\"g\",\"f\",\"v\",\"θ\",\"ð\",\"s\",\"z\",\"ʃ\",\"ʒ\",\"h\",\"m\",\"n\",\"l\",\"r\",\"w\",\"j\",\"tʃ\",\"dʒ\"}\n",
    "ALLOWED_ONSET_2 = {(\"s\",\"p\"),(\"s\",\"t\"),(\"s\",\"k\"),(\"s\",\"m\"),(\"s\",\"n\"),(\"s\",\"w\"),(\"s\",\"l\"),\n",
    "                   (\"p\",\"l\"),(\"p\",\"r\"),(\"b\",\"l\"),(\"b\",\"r\"),(\"t\",\"r\"),(\"t\",\"w\"),(\"d\",\"r\"),\n",
    "                   (\"k\",\"l\"),(\"k\",\"r\"),(\"g\",\"l\"),(\"g\",\"r\"),(\"f\",\"l\"),(\"f\",\"r\"),(\"θ\",\"r\"),(\"ʃ\",\"r\")}\n",
    "ALLOWED_CODA_1 = {\"p\",\"b\",\"t\",\"d\",\"k\",\"g\",\"f\",\"v\",\"θ\",\"ð\",\"s\",\"z\",\"ʃ\",\"ʒ\",\"m\",\"n\",\"ŋ\",\"l\",\"r\",\"tʃ\",\"dʒ\"}\n",
    "ALLOWED_CODA_2 = {(\"p\",\"s\"),(\"t\",\"s\"),(\"k\",\"s\"),(\"g\",\"z\"),(\"f\",\"t\"),(\"f\",\"s\"),\n",
    "                   (\"m\",\"p\"),(\"n\",\"t\"),(\"n\",\"d\"),(\"ŋ\",\"k\"),(\"ŋ\",\"g\"),\n",
    "                   (\"l\",\"d\"),(\"l\",\"t\"),(\"l\",\"k\"),(\"l\",\"f\"),(\"l\",\"z\"),\n",
    "                   (\"r\",\"d\"),(\"r\",\"t\"),(\"r\",\"k\"),(\"r\",\"z\"),(\"r\",\"s\")}\n",
    "BANNED = {\"schnee\",\"meh\",\"heh\",\"uh\",\"ugh\",\"yah\",\"ya\",\"yo\",\"bo\",\"shwa\",\"shwaa\",\"umm\",\"mmm\"}\n",
    "\n",
    "def good_shape(p: Sequence[str]) -> bool:\n",
    "    onset, j, coda = onset_coda(p)\n",
    "    if j is None or len(onset)>2 or len(coda)>2: return False\n",
    "    ok_on = (len(onset)==0) or (len(onset)==1 and onset[0] in ALLOWED_ONSET_1) \\\n",
    "            or (len(onset)==2 and tuple(onset) in ALLOWED_ONSET_2)\n",
    "    ok_cd = (len(coda)==0) or (len(coda)==1 and coda[0] in ALLOWED_CODA_1) \\\n",
    "            or (len(coda)==2 and tuple(coda) in ALLOWED_CODA_2)\n",
    "    return ok_on and ok_cd\n",
    "\n",
    "@dataclass\n",
    "class EnCand:\n",
    "    word: str\n",
    "    pron: Tuple[str,...]\n",
    "    zipf: float\n",
    "    syll: int\n",
    "    length: int\n",
    "\n",
    "cmu = load_cmudict(min_zipf=MIN_ZIPF, max_words=None)\n",
    "EN_CANDS: List[EnCand] = []\n",
    "for w, prons in cmu.items():\n",
    "    if not prons: continue\n",
    "    p = prons[0]\n",
    "    if not re.fullmatch(r\"[a-z']+\", w or \"\"): continue\n",
    "    if w in BANNED: continue\n",
    "    s = syllables(p); L = len(p)\n",
    "    if s > 2 or not (2 <= L <= 7): continue\n",
    "    if not good_shape(p): continue\n",
    "    EN_CANDS.append(EnCand(w, tuple(p), zipf_frequency(w,\"en\"), s, L))\n",
    "\n",
    "EN_CANDS.sort(key=lambda x: (-x.zipf, x.syll, x.length))\n",
    "\n",
    "def en_candidates_for_n(n: int, hu_len: int) -> List[EnCand]:\n",
    "    minL, maxL = LEN_RANGE_FOR_N.get(n, (2,4))\n",
    "    allow2 = ALLOW_2SYLL_FOR_N.get(n, False)\n",
    "    out = []\n",
    "    for c in EN_CANDS:\n",
    "        if c.length < minL or c.length > maxL: \n",
    "            continue\n",
    "        if (c.syll > 1) and not allow2:\n",
    "            continue\n",
    "        out.append(c)\n",
    "    out.sort(key=lambda x: (abs(x.length - hu_len), x.syll, -x.zipf, x.length))\n",
    "    return out[:MAX_CAND] if MAX_CAND else out\n",
    "\n",
    "len(EN_CANDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516906f2",
   "metadata": {},
   "source": [
    "## Costs (proxy‑aware, length/rarity penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d25a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VOWELS_EN = {\"i\",\"ɪ\",\"eɪ\",\"ɛ\",\"æ\",\"ɑ\",\"ʌ\",\"ɝ\",\"ə\",\"oʊ\",\"ɔ\",\"ʊ\",\"u\",\"aɪ\",\"aʊ\",\"ɔɪ\"}\n",
    "def isv_en(p): return p in VOWELS_EN\n",
    "\n",
    "def phone_cost(en_p: str, hu_p: str, alpha=1.5, cv_pen=1.2) -> float:\n",
    "    best = math.inf\n",
    "    for en_proxy, pen in proxies_for_hu_phone(hu_p):\n",
    "        d = phone_distance(en_proxy, en_p)\n",
    "        if isv_en(en_proxy) != isv_en(en_p):\n",
    "            d += cv_pen\n",
    "        cand = alpha*d + pen\n",
    "        if cand < best:\n",
    "            best = cand\n",
    "    return float(best)\n",
    "\n",
    "def unit_cost_base(en_unit: Tuple[str,...], hu_unit: Tuple[str,...]) -> float:\n",
    "    if len(en_unit) == len(hu_unit):\n",
    "        if not hu_unit: return 0.0\n",
    "        return sum(phone_cost(e,h) for e,h in zip(en_unit, hu_unit)) / len(hu_unit)\n",
    "    m = min(len(en_unit), len(hu_unit))\n",
    "    if m == 0:\n",
    "        return 2.0 + LENGTH_GAP_W * abs(len(en_unit) - len(hu_unit))\n",
    "    base = sum(phone_cost(en_unit[i], hu_unit[i]) for i in range(m)) / m\n",
    "    return base + LENGTH_GAP_W * abs(len(en_unit) - len(hu_unit))\n",
    "\n",
    "def rarity_penalty(z: float) -> float:\n",
    "    return max(0.0, 4.0 - float(z)) * RARITY_W\n",
    "\n",
    "def unit_cost(en_unit: Tuple[str,...], hu_unit: Tuple[str,...], z: float) -> float:\n",
    "    return unit_cost_base(en_unit, hu_unit) + EN_LEN_W * len(en_unit) + rarity_penalty(z)\n",
    "\n",
    "def hu_text_to_ipa_tokens(text: str) -> Tuple[str,...]:\n",
    "    iph = [p for p in hu_text_to_ipa(text) if isinstance(p, str) and p]\n",
    "    return tuple(iph)\n",
    "\n",
    "def best_en_for_ngram(morphs: Tuple[str,...]) -> Tuple[str, Tuple[str,...], float]:\n",
    "    hu_text = \"\".join(morphs)\n",
    "    hu_ipa  = hu_text_to_ipa_tokens(hu_text)\n",
    "    if not hu_ipa:\n",
    "        return \"\", tuple(), float(\"inf\")\n",
    "    cand_list = en_candidates_for_n(len(morphs), len(hu_ipa))\n",
    "    best_w, best_p, best_c = \"\", tuple(), float(\"inf\")\n",
    "    for c in cand_list:\n",
    "        cst = unit_cost(c.pron, hu_ipa, c.zipf)\n",
    "        if cst < best_c:\n",
    "            best_c = cst; best_w = c.word; best_p = c.pron\n",
    "    return best_w, best_p, best_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d3d58",
   "metadata": {},
   "source": [
    "## Build mapping for top uni/bi/tri‑grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2829df06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: data/coverage/morph_ngram_map.csv rows: 8567\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hu_ngram</th>\n",
       "      <th>n</th>\n",
       "      <th>en_word</th>\n",
       "      <th>en_pron</th>\n",
       "      <th>cost</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>et</td>\n",
       "      <td>1</td>\n",
       "      <td>et</td>\n",
       "      <td>ɛ t</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>el</td>\n",
       "      <td>1</td>\n",
       "      <td>l</td>\n",
       "      <td>ɛ l</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>n</td>\n",
       "      <td>ɛ n</td>\n",
       "      <td>0.16</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>em</td>\n",
       "      <td>1</td>\n",
       "      <td>m</td>\n",
       "      <td>ɛ m</td>\n",
       "      <td>0.16</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an</td>\n",
       "      <td>1</td>\n",
       "      <td>on</td>\n",
       "      <td>ɑ n</td>\n",
       "      <td>0.16</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ad</td>\n",
       "      <td>1</td>\n",
       "      <td>odd</td>\n",
       "      <td>ɑ d</td>\n",
       "      <td>0.16</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ad</td>\n",
       "      <td>1</td>\n",
       "      <td>odd</td>\n",
       "      <td>ɑ d</td>\n",
       "      <td>0.16</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ot</td>\n",
       "      <td>1</td>\n",
       "      <td>ought</td>\n",
       "      <td>ɔ t</td>\n",
       "      <td>0.16</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ed</td>\n",
       "      <td>1</td>\n",
       "      <td>ed</td>\n",
       "      <td>ɛ d</td>\n",
       "      <td>0.16</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in</td>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>ɪ n</td>\n",
       "      <td>0.16</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ol</td>\n",
       "      <td>1</td>\n",
       "      <td>all</td>\n",
       "      <td>ɔ l</td>\n",
       "      <td>0.16</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>az</td>\n",
       "      <td>1</td>\n",
       "      <td>oz</td>\n",
       "      <td>ɑ z</td>\n",
       "      <td>0.16</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>egy</td>\n",
       "      <td>1</td>\n",
       "      <td>edge</td>\n",
       "      <td>ɛ dʒ</td>\n",
       "      <td>0.16</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>it</td>\n",
       "      <td>1</td>\n",
       "      <td>it</td>\n",
       "      <td>ɪ t</td>\n",
       "      <td>0.16</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ön</td>\n",
       "      <td>1</td>\n",
       "      <td>own</td>\n",
       "      <td>oʊ n</td>\n",
       "      <td>0.16</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hu_ngram  n en_word en_pron  cost  count\n",
       "0        et  1      et     ɛ t  0.16   1794\n",
       "1        el  1       l     ɛ l  0.16   1425\n",
       "2        en  1       n     ɛ n  0.16    858\n",
       "3        em  1       m     ɛ m  0.16    600\n",
       "4        an  1      on     ɑ n  0.16    522\n",
       "5        ad  1     odd     ɑ d  0.16    512\n",
       "6        Ad  1     odd     ɑ d  0.16    416\n",
       "7        ot  1   ought     ɔ t  0.16    415\n",
       "8        ed  1      ed     ɛ d  0.16    329\n",
       "9        in  1      in     ɪ n  0.16    293\n",
       "10       ol  1     all     ɔ l  0.16    279\n",
       "11       az  1      oz     ɑ z  0.16    245\n",
       "12      egy  1    edge    ɛ dʒ  0.16    231\n",
       "13       it  1      it     ɪ t  0.16    135\n",
       "14       ön  1     own    oʊ n  0.16    122"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def parse_units(df: pd.DataFrame) -> List[Tuple[Tuple[str,...], int]]:\n",
    "    out = []\n",
    "    for _,r in df.iterrows():\n",
    "        u = tuple(str(r[\"unit\"]).split(\" + \"))\n",
    "        c = int(r.get(\"count\", 1))\n",
    "        out.append((u,c))\n",
    "    return out\n",
    "\n",
    "rows = []\n",
    "for n, dftop in [(1, top1), (2, top2), (3, top3)]:\n",
    "    items = parse_units(dftop)\n",
    "    for u, c in items:\n",
    "        w, p, cost = best_en_for_ngram(u)\n",
    "        rows.append({\n",
    "            \"hu_ngram\": \" + \".join(u),\n",
    "            \"n\": n,\n",
    "            \"en_word\": w,\n",
    "            \"en_pron\": \" \".join(p),\n",
    "            \"cost\": round(cost, 3),\n",
    "            \"count\": c,\n",
    "        })\n",
    "\n",
    "map_df = pd.DataFrame(rows).sort_values([\"n\",\"cost\",\"count\"], ascending=[True, True, False]).reset_index(drop=True)\n",
    "map_df.to_csv(CSV_MAP, index=False)\n",
    "print(\"Wrote:\", CSV_MAP, \"rows:\", len(map_df))\n",
    "map_df.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad30637a",
   "metadata": {},
   "source": [
    "## DP translator (choose 1/2/3‑gram mappings per context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9f1d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turn you l, turn you l tell lodge oz blocks been well. blood.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "WORD_RE = re.compile(r\"\\w+|[^\\w\\s]\", re.UNICODE)\n",
    "\n",
    "def load_ngram_map(csv_path: str | Path) -> Dict[int, Dict[Tuple[str,...], Tuple[str, Tuple[str,...], float]]]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    byn: Dict[int, Dict[Tuple[str,...], Tuple[str, Tuple[str,...], float]]] = {1:{},2:{},3:{}}\n",
    "    for _, r in df.iterrows():\n",
    "        n = int(r[\"n\"]); u = tuple(str(r[\"hu_ngram\"]).split(\" + \"))\n",
    "        w = str(r[\"en_word\"]); p = tuple(str(r[\"en_pron\"]).split())\n",
    "        c = float(r[\"cost\"])\n",
    "        byn[n][u] = (w, p, c)\n",
    "    return byn\n",
    "\n",
    "def dp_translate_tokens(morphs: List[str], ngram_map: Dict[int, Dict[Tuple[str,...], Tuple[str, Tuple[str,...], float]]]):\n",
    "    N = len(morphs)\n",
    "    dp = [ (float(\"inf\"), []) for _ in range(N+1) ]\n",
    "    dp[0] = (0.0, [])\n",
    "    for i in range(N):\n",
    "        cur_cost, cur_path = dp[i]\n",
    "        if not math.isfinite(cur_cost): continue\n",
    "        for k in range(1, NGRAM_MAX+1):\n",
    "            j = i + k\n",
    "            if j > N: break\n",
    "            u = tuple(morphs[i:j])\n",
    "            if u in ngram_map.get(k, {}):\n",
    "                w, pr, c = ngram_map[k][u]\n",
    "            else:\n",
    "                w, pr, c = best_en_for_ngram(u)\n",
    "            new_cost = cur_cost + c + STEP_PENALTY\n",
    "            if new_cost < dp[j][0]:\n",
    "                dp[j] = (new_cost, cur_path + [w if w else \"uh\"])\n",
    "    return dp[N]\n",
    "\n",
    "def translate_text(text: str, ngram_map_csv: str | Path, emtsv_exec: str = EMTSV_EXEC) -> str:\n",
    "    nmap = load_ngram_map(ngram_map_csv)\n",
    "    out = []\n",
    "    for tok in WORD_RE.findall(text):\n",
    "        if not tok.strip() or not tok.isalpha():\n",
    "            out.append(tok); continue\n",
    "        morphs = morph_segments_for_text(tok, emtsv_exec=emtsv_exec)\n",
    "        cost, words = dp_translate_tokens(morphs, nmap)\n",
    "        out.append(\" \".join(words))\n",
    "    s = \" \".join(out)\n",
    "    s = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", s)\n",
    "    s = re.sub(r\"\\(\\s+\", \"(\", s); s = re.sub(r\"\\s+\\)\", \")\", s)\n",
    "    return s\n",
    "\n",
    "# Demo (requires emtsv in your environment)\n",
    "try:\n",
    "    print(translate_text(\"Tűnj el, tűnj el te vagy az igazi bűnjel. Áradás.\", CSV_MAP, emtsv_exec=EMTSV_EXEC))\n",
    "except Exception as e:\n",
    "    print(\"Demo skipped (needs emtsv).\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403231e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
